# -*- coding: utf-8 -*-
"""COMP 4980 - ST: ML Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pdgZObZjJfLVkvCbDD7iIMzkiNkPyyuT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import csv
import io

from google.colab import files
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from scipy import stats
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier 
from sklearn.decomposition import PCA
from sklearn.model_selection import cross_val_score
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import LeavePOut
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import sklearn.ensemble as ens
import sklearn.model_selection as ms
from sklearn.ensemble import AdaBoostClassifier
import sklearn.neural_network as nn
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

file = files.upload()

"""# **1. Data Analysis**"""

df = pd.read_csv('survey.csv')
df.head(3)

df.shape

df.info()

df = df.drop(labels=["Timestamp","state", "no_employees", "comments"], axis=1)

pd.options.display.max_rows = 200
df.isna().sum(axis = 0)

df.Gender.value_counts()

# Clean gender's and make them either Male, Female, Non-binary


df["Gender"].replace(
    [
        "Cis Male", "Cis Man", "M", "Mail", "Make", "Mal", "Male ","Male (CIS)",
        "Male-ish", "Man", "m", "cis male", "maile", "male", "msle","Malr",
    ],
    "Male", inplace=True,
)

df["Gender"].replace(
    [
        "Cis Female", "F", "Femake", "Female ", "Female (cis)", "Woman",
        "femail", "female", "woman", "cis-female/femme", "f",
    ],
    "Female", inplace=True,
)

df["Gender"].replace(
    [
        "A little about you", "Agender", "All", "Androgyne", "Enby",
        "non-binary", "Nah", "something kinda male?", "p",
        "ostensibly male, unsure what that really means", "Genderqueer",
        "queer/she/they", "Neuter", "Trans woman", "Trans-female", "queer",
        "fluid", "fluid", "male leaning androgynous", "Female (trans)",
        "Guy (-ish) ^_^",
    ],
    "Nonbinary", inplace=True,
)

df.Gender.value_counts()

plt.figure(figsize = (8,5))
sns.countplot(x = 'Gender', data = df, hue = 'treatment')
plt.show()

print("Age", "\t", "Number of People")
df.Age.value_counts()

df['Age'] = df['Age'].astype(int)

print("Age Outliers \n")
for i in range(len(df['Age'])):
  if(df['Age'][i] < 14 or df['Age'][i] >100):
    print(df['Age'][i])

for i in range(len(df['Age'])):
  if(df['Age'][i] < 14 or df['Age'][i] >100):
    df.replace(to_replace=df['Age'][i], value = 29, inplace=True)

print("Age", "\t", "Number of People")
df.Age.value_counts()

df.self_employed.value_counts()

pd.options.display.max_rows = 200
df.isna().sum(axis = 0)

"""
for i in range(len(df['self_employed'])):
  if(df['self_employed'][i] != "Yes" and df['self_employed'][i] != "No"):
    #print(df['self_employed'][i])
  #if(df['self_employed'][i] == "NA"):
    df.replace(to_replace=df['self_employed'][i], value = "Unknown", inplace=True)

print("Self-emp", " ", "Number of people")
df.self_employed.value_counts()
"""

df['self_employed'] = df['self_employed'].fillna(value="Don't know")

pd.options.display.max_rows = 200
df.isna().sum(axis = 0)

df.work_interfere.value_counts()

pd.options.display.max_rows = 200
df.isna().sum(axis = 0)

df['work_interfere'] = df['work_interfere'].fillna(value="Don't know")

pd.options.display.max_rows = 200
df.isna().sum(axis = 0)

df.head()

df.self_employed.value_counts()

df.family_history.value_counts()

df.treatment.value_counts()

df.work_interfere.value_counts()

df.leave.value_counts()

df.Country.value_counts()

plt.figure(figsize = (8,5))
sns.countplot(x = 'treatment', data = df)
plt.show()

plt.figure(figsize = (8,5))
sns.countplot(x = 'family_history', data = df, hue = 'treatment')
plt.show()

plt.figure(figsize = (25,5))
sns.countplot(x = 'Country', data = df, hue = 'treatment')
plt.xticks(rotation=90)
plt.show()

df.replace(to_replace=['Nonbinary'], value="0", inplace=True) # 0 = Non-binary
df.replace(to_replace=['Female'], value="1", inplace=True) # 1 = Female
df.replace(to_replace=['Male'], value="2", inplace=True) # 2 = Male

df.replace(to_replace=["Don't know"], value="0", inplace=True) # 0 = Unknown
df.replace(to_replace=["Not sure"], value="0", inplace=True) # 0 = Not sure
df.replace(to_replace=["Maybe"], value="0", inplace=True)
df.replace(to_replace=['Yes'], value="1", inplace=True) # 1 = Yes
df.replace(to_replace=['No'], value="2", inplace=True) # 2 = No

df.replace(to_replace=['Never'], value="1", inplace=True) # 1 = Never
df.replace(to_replace=['Rarely'], value="2", inplace=True) # 2 = Rarely
df.replace(to_replace=['Sometimes'], value="3", inplace=True) # 3 = Sometimes
df.replace(to_replace=['Often'], value="4", inplace=True) # 4 = Often

df.replace(to_replace=['Somewhat easy'], value="1", inplace=True)
df.replace(to_replace=['Very easy'], value="2", inplace=True)
df.replace(to_replace=['Somewhat difficult'], value="3", inplace=True)
df.replace(to_replace=['Very difficult'], value="4", inplace=True)

df.replace(to_replace=['Some of them'], value="3", inplace=True)

df.replace(to_replace=['United States'], value="1", inplace=True)
df.replace(to_replace=['Canada'], value="1", inplace=True)
df.replace(to_replace=['United Kingdom'], value="2", inplace=True)
df.replace(to_replace=['Germany'], value="2", inplace=True)
df.replace(to_replace=['Ireland'], value="2", inplace=True)
df.replace(to_replace=['Netherlands'], value="2", inplace=True)
df.replace(to_replace=['Australia'], value="2", inplace=True)
df.replace(to_replace=['France'], value="2", inplace=True)
df.replace(to_replace=['India'], value="2", inplace=True)
df.replace(to_replace=['New Zealand'], value="2", inplace=True)
df.replace(to_replace=['Switzerland'], value="2", inplace=True)
df.replace(to_replace=['Italy'], value="2", inplace=True)
df.replace(to_replace=['Sweden'], value="2", inplace=True)
df.replace(to_replace=['Poland'], value="2", inplace=True)
df.replace(to_replace=['Brazil'], value="2", inplace=True)
df.replace(to_replace=['Belgium'], value="2", inplace=True)
df.replace(to_replace=['South Africa'], value="2", inplace=True)
df.replace(to_replace=['Israel'], value="2", inplace=True)
df.replace(to_replace=['Bulgaria'], value="2", inplace=True)
df.replace(to_replace=['Singapore'], value="2", inplace=True)
df.replace(to_replace=['Finland'], value="2", inplace=True)
df.replace(to_replace=['Russia'], value="2", inplace=True)
df.replace(to_replace=['Mexico'], value="2", inplace=True)
df.replace(to_replace=['Austria'], value="2", inplace=True)
df.replace(to_replace=['Portugal'], value="2", inplace=True)
df.replace(to_replace=['Greece'], value="2", inplace=True)
df.replace(to_replace=['Colombia'], value="2", inplace=True)
df.replace(to_replace=['Croatia'], value="2", inplace=True)
df.replace(to_replace=['Denmark'], value="2", inplace=True)
df.replace(to_replace=['Zimbabwe'], value="2", inplace=True)
df.replace(to_replace=['Thailand'], value="2", inplace=True)
df.replace(to_replace=['Romania'], value="2", inplace=True)
df.replace(to_replace=['Japan'], value="2", inplace=True)
df.replace(to_replace=['Moldova'], value="2", inplace=True)
df.replace(to_replace=['Latvia'], value="2", inplace=True)
df.replace(to_replace=['Nigeria'], value="2", inplace=True)
df.replace(to_replace=['Philippines'], value="2", inplace=True)
df.replace(to_replace=['Georgia'], value="2", inplace=True)
df.replace(to_replace=['Spain'], value="2", inplace=True)
df.replace(to_replace=['China'], value="2", inplace=True)
df.replace(to_replace=['Uruguay'], value="2", inplace=True)
df.replace(to_replace=['Norway'], value="2", inplace=True)
df.replace(to_replace=['Costa Rica'], value="2", inplace=True)
df.replace(to_replace=['Bosnia and Herzegovina'], value="2", inplace=True)
df.replace(to_replace=['Bahamas, The'], value="2", inplace=True)
df.replace(to_replace=['Czech Republic'], value="2", inplace=True)
df.replace(to_replace=['Slovenia'], value="2", inplace=True)
df.replace(to_replace=['Hungary'], value="2", inplace=True)

df.head()

df.shape

df['Age'] = df['Age'].astype(int)
df['Gender'] = df['Gender'].astype(int)
df['Country'] = df['Country'].astype(int)
df['self_employed'] = df['self_employed'].astype(int)
df['family_history'] = df['family_history'].astype(int)
df['treatment'] = df['treatment'].astype(int)
df['work_interfere'] = df['work_interfere'].astype(int)
df['remote_work'] = df['remote_work'].astype(int)
df['tech_company'] = df['tech_company'].astype(int)
df['benefits'] = df['benefits'].astype(int)
df['care_options'] = df['care_options'].astype(int)
df['wellness_program'] = df['wellness_program'].astype(int)
df['seek_help'] = df['seek_help'].astype(int)
df['anonymity'] = df['anonymity'].astype(int)
df['leave'] = df['leave'].astype(int)
df['mental_health_consequence'] = df['mental_health_consequence'].astype(int)
df['phys_health_consequence'] = df['phys_health_consequence'].astype(int)
df['coworkers'] = df['coworkers'].astype(int)
df['supervisor'] = df['supervisor'].astype(int)
df['mental_health_interview'] = df['mental_health_interview'].astype(int)
df['phys_health_interview'] = df['phys_health_interview'].astype(int)
df['mental_vs_physical'] = df['mental_vs_physical'].astype(int)
df['obs_consequence'] = df['obs_consequence'].astype(int)

plt.figure(figsize=(8,5))
sns.distplot(df["Age"], bins=25)
plt.title("Age Distribution")
plt.xlabel("Age")
plt.show

g = sns.FacetGrid(df, col='treatment', size=5)
g.map(sns.distplot, "Age")

plt.figure(figsize=(18, 12))
heatmap = sns.heatmap(df.corr(), annot = True, linewidths = .5, cmap = "YlGnBu", fmt = '.1f')
heatmap.set_title('Correlation Heatmap');

plt.figure(figsize=(10, 8))
heatmap = sns.heatmap(df.corr()[['treatment']].sort_values(by='treatment'), annot=True, cmap='BrBG')
heatmap.set_title('Features Correlating with treatment', fontdict={'fontsize':18}, pad=16);

"""# **4. Data Exploration**

## 1. Principle Component Analysis (PCA)
"""

inputs = ['family_history', 'Gender', 'obs_consequence', 'Country',
               'mental_health_consequence', 'phys_health_consequence', 
               'coworkers', 'remote_work', 'care_options', 'self_employed',
               'wellness_program', 'seek_help', 'supervisor', 'tech_company',
               'phys_health_interview', 'Age', 'mental_health_interview',
               'benefits', 'mental_vs_physical', 'anonymity', 'leave',
               'work_interfere']
output = 'treatment'

X = df.loc[:, inputs].values
X.shape

y = df.loc[:, output].values
y.shape

standard_scaler = StandardScaler()
standard_scaler.fit(X)
X = standard_scaler.transform(X)

pca = PCA(22)
pca.fit(X)
X = pca.transform(X)

print(pca.n_components_)

#pca.components_

pca.explained_variance_ratio_

"""11.8% variance in the data shown by the first component. Other components have very less effect on the variance...9.91% variance shown by the second component, 7.70% shown by the third one. So on and so forth. Since we do not have a component that is responsible for explaining a vast majority of the variance, we won't drop any field.

## 2. Classification Decision Trees
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

y

y_train

y_test

clf = DecisionTreeClassifier(max_depth=5)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(accuracy_score(y_test, y_pred))

clf = DecisionTreeClassifier(max_depth = 3)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
plt.figure(figsize=(20, 8))
var = tree.plot_tree(clf, feature_names=inputs, class_names=str(y),
                     filled = True, fontsize = 9.5)



"""# **5. Experimental Method**

# 1. Support Vector Classifier
"""

k = 10 
cv = RepeatedKFold(n_splits=k, n_repeats=5)
clf = SVC(kernel='linear')
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 10
cv = KFold(n_splits=k, shuffle=True)
clf = SVC(C = 10.0, kernel='linear')
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 10
cv = KFold(n_splits=k, shuffle=True)
clf = SVC(C = 5.0, kernel='linear')
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

"""# 2. Classification Decision Trees"""

cv = LeaveOneOut()
clf = DecisionTreeClassifier()
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv=cv, n_jobs=-1)

print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 7
cv = KFold(n_splits=k, shuffle=True)
clf = DecisionTreeClassifier(min_samples_split=20, random_state=99)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print(f'Scores for each fold are: {scores}')
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 10
cv = RepeatedKFold(n_splits=k)
clf = DecisionTreeClassifier(min_samples_split=40, random_state=99)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

"""# 3. Random Forest Classifier"""

k = 13
cv = KFold(n_splits=k, shuffle=True)
clf = RandomForestClassifier(random_state=99)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 10
cv = RepeatedKFold(n_splits=k)
clf = RandomForestClassifier(random_state=99)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 13
cv = RepeatedKFold(n_splits=k)
clf = RandomForestClassifier(random_state=99)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf = RandomForestClassifier(random_state=99)
param_grid = { 
    'n_estimators': [200],
    'max_features': [10],
    'max_depth' : [10],
    'criterion' :['gini']
}
clf_gridSearch = GridSearchCV(estimator=clf, param_grid=param_grid, cv= cv)
clf_gridSearch.fit(X_train, y_train)

k = 13
cv = RepeatedKFold(n_splits=k)
clf = RandomForestClassifier(n_estimators = 200, criterion='gini', max_features = 10,
                             max_depth = 10)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf = RandomForestClassifier(random_state=99)
param_grid = { 
    'n_estimators': [100],
    'max_features': [22],
    'max_depth' : [7,10],
    'criterion' :['gini','entropy']
}
clf_gridSearch = GridSearchCV(estimator=clf, param_grid=param_grid, cv= cv)
clf_gridSearch.fit(X_train, y_train)

clf_gridSearch.best_params_

k = 13
cv = RepeatedKFold(n_splits=k)
clf = RandomForestClassifier(n_estimators = 100, criterion='entropy', max_features = 22,
                             max_depth = 10, random_state = 99)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 13
cv = RepeatedKFold(n_splits=k)
clf = RandomForestClassifier(n_estimators = 300, criterion='entropy', random_state = 99)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf = RandomForestClassifier(random_state=99)
param_grid = {
    'max_depth' : [10, 12],
    'n_estimators': [300, 500],
    'criterion' :['gini','entropy']
}
clf_gridSearch = GridSearchCV(estimator=clf, param_grid=param_grid, cv= cv)
clf_gridSearch.fit(X_train, y_train)

clf_gridSearch.best_params_

k = 13
cv = RepeatedKFold(n_splits=k)
clf = RandomForestClassifier(n_estimators = 500, criterion='entropy',
                             random_state = 99, max_depth=10)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 13
cv = KFold(n_splits=k, shuffle=True)
clf = RandomForestClassifier(n_estimators = 600, criterion='entropy',
                             random_state = 99, max_depth=10)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

"""#4. Adaboost"""

k = 13
cv = RepeatedKFold(n_splits=k)
clf = AdaBoostClassifier()
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf = AdaBoostClassifier()
param_grid = {
    'learning_rate': [0.1, 0.5, 0.2, 0.3]
}
clf_gridSearch = GridSearchCV(estimator=clf, param_grid=param_grid, cv= cv)
clf_gridSearch.fit(X_train, y_train)

clf_gridSearch.best_params_

k = 13
cv = RepeatedKFold(n_splits=k)
clf = AdaBoostClassifier(learning_rate=0.3)
clf.fit(X, y)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

"""# 5. Multi-Layer Perception (MLP)"""

k = 13
cv = ms.StratifiedKFold()
clf = nn.MLPClassifier(hidden_layer_sizes=(16,16,4), activation='relu',
                       solver='adam', alpha=0.0001)
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
cv = ms.StratifiedKFold()
clf = nn.MLPClassifier(solver='adam')
param_grid = {
    'hidden_layer_sizes': [(16, 16, 4), (20, 15, 10, 5), (20, 17, 13, 10, 8, 5),
                           (18, 15, 10, 7, 5)],
    'activation': ['relu', 'sigmoid'],
    'alpha' : [0.01, 0.02, 0.001, 0.00002, 0.0001]
}
clf_gridSearch = GridSearchCV(estimator=clf, param_grid=param_grid, cv= cv)
clf_gridSearch.fit(X_train, y_train)

clf_gridSearch.best_params_

k = 13
cv = ms.StratifiedKFold()
clf = nn.MLPClassifier(hidden_layer_sizes=(16, 16, 4), activation='relu',
                       alpha=0.0001, solver='adam')
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 13
cv = ms.StratifiedKFold()
clf = nn.MLPClassifier(hidden_layer_sizes=(20, 15, 10, 5), activation='relu', alpha=0.01, solver='adam')
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

k = 13
cv = ms.StratifiedKFold()
clf = nn.MLPClassifier(hidden_layer_sizes=(16, 16, 4), activation='relu', alpha=0.02, solver='adam')
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
cv = ms.StratifiedKFold()
clf = nn.MLPClassifier(solver='adam')
param_grid = {
    'hidden_layer_sizes': [(16, 16, 4), (20, 15, 10, 5), (20, 17, 13, 10, 8, 5), (18, 15, 10, 7, 5)],
    'activation': ['relu', 'sigmoid'],
    'alpha' : [0.01, 0.02, 0.001, 0.00002, 0.0001],
    'learning_rate' : ['constant', 'adaptive']
}
clf_gridSearch = GridSearchCV(estimator=clf, param_grid=param_grid, cv= cv)
clf_gridSearch.fit(X_train, y_train)

clf_gridSearch.best_params_

k = 13
cv = ms.StratifiedKFold()
clf = nn.MLPClassifier(hidden_layer_sizes=(18, 15, 10, 7, 5), activation='relu', alpha=0.02, solver='adam', learning_rate='adaptive')
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
models = ['KNN', 'SVC','Random Forest', 'Decision Trees', 'Bagging Classifier',
          'Random Forest Classifier', 'Ada Boost Classifier',
          'Gradient Boost Classifier']
clf = [KNeighborsClassifier(10),
       SVC(kernel='linear'),
       RandomForestClassifier(random_state=99),
       DecisionTreeClassifier(max_depth=4),
       ens.BaggingClassifier(),
       ens.RandomForestClassifier(),
       ens.AdaBoostClassifier(),
       ens.GradientBoostingClassifier()]
i=0;
while(i <= 6):
  for c in clf:
    c.fit(X_train, y_train)
    y_pred = c.predict(X_test)
    print(models[i], "accuracy: ", accuracy_score(y_test,y_pred))
    i += 1

"""## Stratified K Fold"""

models = ['KNN', 'SVC','Random Forest', 'Decision Trees', 'Bagging Classifier',
           'Ada Boost Classifier','Gradient Boost Classifier',
          'Multi-Layer Perceptron']
clf = [KNeighborsClassifier(10),
       SVC(kernel='linear'),
       RandomForestClassifier(random_state=99),
       DecisionTreeClassifier(min_samples_split=40, random_state=99),
       ens.BaggingClassifier(),
       ens.AdaBoostClassifier(learning_rate=0.3),
       ens.GradientBoostingClassifier(),
       nn.MLPClassifier(hidden_layer_sizes=(16, 16, 4), activation='relu',
                        solver = 'adam', alpha=0.0001)]
cv = ms.StratifiedKFold()

i = 0;
while(i<6):
  for c in clf:
    scores = ms.cross_val_score(c, X, y, cv=cv, n_jobs=-1)
    print(models[i], "Mean : ", np.mean(scores))
    i += 1

"""KFold

# Repeated KFold
"""

models = ['KNN', 'SVC','Random Forest', 'Decision Trees', 'Bagging Classifier',
           'Ada Boost Classifier','Gradient Boost Classifier',
          'Multi-Layer Perceptron']
clf = [KNeighborsClassifier(10),
       SVC(kernel='linear'),
       RandomForestClassifier(random_state=99),
       DecisionTreeClassifier(min_samples_split=40, random_state=99),
       ens.BaggingClassifier(),
       ens.AdaBoostClassifier(learning_rate=0.3),
       ens.GradientBoostingClassifier(),
       nn.MLPClassifier(hidden_layer_sizes=(16, 16, 4), activation='relu',
                        solver = 'adam', alpha=0.0001)]

k=13
cv = RepeatedKFold(n_splits=k)

i = 0;
while(i<6):
  for c in clf:
    scores = ms.cross_val_score(c, X, y, cv=cv, n_jobs=-1)
    print(models[i], "Mean : ", np.mean(scores))
    i += 1

models = ['KNN', 'SVC','Random Forest', 'Decision Trees', 'Bagging Classifier',
           'Ada Boost Classifier','Gradient Boost Classifier',
          'Multi-Layer Perceptron']
clf = [KNeighborsClassifier(10),
       SVC(C = 0.5, kernel='linear'),
       RandomForestClassifier(random_state=99),
       DecisionTreeClassifier(min_samples_split=40, random_state=99),
       ens.BaggingClassifier(),
       ens.AdaBoostClassifier(learning_rate=0.3),
       ens.GradientBoostingClassifier(),
       nn.MLPClassifier(hidden_layer_sizes=(16, 16, 4), activation='relu',
                        solver = 'adam', alpha=0.0001)]
cv = ms.StratifiedKFold()

i = 0;
while(i<6):
  for c in clf:
    scores = ms.cross_val_score(c, X, y, cv=cv, n_jobs=-1)
    print(models[i], "Mean : ", np.mean(scores))
    i += 1

"""# **6. Results**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

clf = SVC(C = 5.0, kernel='linear')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print(confusion_matrix(y_test, y_pred))

print(classification_report(y_test, y_pred))

cv = StratifiedKFold()
clf = SVC(C = 5.0, kernel='linear')
scores = cross_val_score(clf, X, y, cv = cv)
print("Mean of the accuracy scores: ", np.mean(scores))
print("Standard Deviation of the accuracy scores: ", np.std(scores))

